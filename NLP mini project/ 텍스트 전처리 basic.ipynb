{"cells":[{"cell_type":"markdown","metadata":{"id":"UKjg766IvMdI"},"source":["# 텍스트 전처리 기초 실습"]},{"cell_type":"markdown","metadata":{"id":"OXUZV0j3nBJd"},"source":["## 실습 개요\n","\n","\n","(실습) 텍스트 전처리 기초 실습하기 <br>\n","1) 실습 목적 <br>\n"," 이번 실습에서는 전반적인 자연언어처리를 시작할때 필수적으로 사용되는 여러 전처리 기술들을 직접 실행해봅니다<br>\n"," Tokenization, Cleaning, Normalization, Stemming, Lemmatization 등을 실습해봅니다\n","\n"," 2) 수강 목표\n","  - 자연언어 처리에서 필요한 전처리 단계들을 이해하고 적용할 수 있다\n","  - 각 과정에서 일어나는 자연언어의 형태 변환에 대해 이해한다\n","  - 분석 목적에 필요한 전처리 방법들을 선택해 적용할 수 있다"]},{"cell_type":"markdown","metadata":{"id":"H5CYPnoeDoRu"},"source":["### 실습 목차\n","\n","* 1. 텍스트 전처리\n","  * 1-1. Tokenization\n","    * 1-1-1. 단어 토큰화\n","    * 1-1-2. 문장 토큰화\n","  * 1-2. Cleaning\n","  * 1-3. Normalization\n","    * 1-3-1. Stemming\n","    * 1-3-2. Lemmatization\n","  * 1-4. Edit Distance\n","  "]},{"cell_type":"markdown","metadata":{"id":"siW2vxW_5R8u"},"source":["## 1. 텍스트 전처리\n","\n","```\n","💡 목차 개요 : 텍스트 전처리의 여러 방법들을 파악하고 직접 적용해봅니다\n","```\n","\n","- 1-1 Tokenization에 대해 이해하고 자연어를 직접 토큰 단위로 분할해봅니다\n","- 1-2 불필요한 텍스트를 정제하거나 제외하는 Cleaning 과정을 수행해봅니다\n","- 1-3 유사한 의미를 갖는 자연언어들을 처리하고 통합하는 과정을 이해합니다\n","- 1-4 문자열 간의 거리 개념과 edit distance가 갖는 연산을 이해합니다\n"]},{"cell_type":"markdown","metadata":{"id":"wOhHH9j78vg0"},"source":["### 1-1. Tokenization\n","\n","> 주어진 데이터를 토큰(Token)이라 불리는 단위로 나누는 작업을 tokenization이라고합니다. 토큰이 되는 기준은 설정하기에 따라 다를 수 있습니다(어절, 단어, 형태소, 음절, 자소 등). 일반적으로 Character-based Tokenization / Word-based Tokenization / Subword-based Tokenization으로 구분됩니다\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"jLRXK4HLrxFA","outputId":"f9a7754a-f7e4-492f-dc3d-68bc9ed97ac1"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]}],"source":["import nltk\n","nltk.download('punkt')\n","\n","from nltk.tokenize import word_tokenize\n","from nltk.tokenize import WordPunctTokenizer\n","from nltk.tokenize import TreebankWordTokenizer"]},{"cell_type":"markdown","metadata":{"id":"1MqwWyulS2UU"},"source":["+ nltk에서는 여러 tokenizer들을 제공합니다. 해당 라이브러리에 있는 함수들을 사용하면 사전에 정의된 tokenizer의 규칙에 따라 쉽게 자연언어 문장을 토큰화할 수 있습니다"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"XjkXZNOHSndh"},"outputs":[],"source":["text = \"You won't be able to know about natural language perfectly, but you need to master a number of preprocessing skills to perform natural language processing.\""]},{"cell_type":"markdown","metadata":{"id":"5vIIjbJCRUi0"},"source":["#### 1-1-1 단어 토큰화"]},{"cell_type":"markdown","metadata":{"id":"BLeskjL4VvGq"},"source":["**word_tokenize**\n","+ nltk의 word_tokenize는 문장을 단어 기반으로 토큰화합니다. 해당 함수는 space와 구두점을 기준으로 토큰화합니다\n","+ 해당 함수는 won't를 wo / n't로 분리하고있는 것을 볼 수 있습니다"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"FfGxHIhXrxCq","outputId":"876fbc7b-8517-4eec-b6e1-e26dd9288948"},"outputs":[{"data":{"text/plain":["['You',\n"," 'wo',\n"," \"n't\",\n"," 'be',\n"," 'able',\n"," 'to',\n"," 'know',\n"," 'about',\n"," 'natural',\n"," 'language',\n"," 'perfectly',\n"," ',',\n"," 'but',\n"," 'you',\n"," 'need',\n"," 'to',\n"," 'master',\n"," 'a',\n"," 'number',\n"," 'of',\n"," 'preprocessing',\n"," 'skills',\n"," 'to',\n"," 'perform',\n"," 'natural',\n"," 'language',\n"," 'processing',\n"," '.']"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["# word_toknize\n","word_tokenize(text)"]},{"cell_type":"markdown","metadata":{"id":"UzW61tNMVsO_"},"source":["**WordPunctTokenizer**\n","+ WordPunctTokenizer는 word_tokenize에 대한 대안으로써 구두점을 별도로 분류하는 특징을 갖고 있기때문에, 앞서 확인했던 word_tokenize와는 달리 won't를 won과 t로 분리합니다"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Qca-xElYRZaU","outputId":"6d2172fe-a336-44fc-dcb0-67e735af0a46"},"outputs":[{"data":{"text/plain":["['You',\n"," 'won',\n"," \"'\",\n"," 't',\n"," 'be',\n"," 'able',\n"," 'to',\n"," 'know',\n"," 'about',\n"," 'natural',\n"," 'language',\n"," 'perfectly',\n"," ',',\n"," 'but',\n"," 'you',\n"," 'need',\n"," 'to',\n"," 'master',\n"," 'a',\n"," 'number',\n"," 'of',\n"," 'preprocessing',\n"," 'skills',\n"," 'to',\n"," 'perform',\n"," 'natural',\n"," 'language',\n"," 'processing',\n"," '.']"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# WordPunctTokenizer\n","punct_tokenizer = WordPunctTokenizer() # 토크나이저를 선언\n","\n","punct_tokenizer.tokenize(text)"]},{"cell_type":"markdown","metadata":{"id":"sIjcoZbSVrJi"},"source":["**TreebankWordTokenizer**\n","+ TreebankWordTokenizer는 가장 표준적인 방법의 토큰화 방법으로 사용되는 tokenizer입니다\n","+ 해당 토크나이저는 두가지 규칙을 갖습니다\n","  + 규칙 1. 하이푼으로 구성된 단어는 하나로 유지한다\n","  + 규칙 2. doesn't와 같이 아포스트로피로 '접어'가 함께하는 단어는 분리해준다"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wzeKjl8PUhf7"},"outputs":[],"source":["tree_tokenizer = TreebankWordTokenizer() # 토크나이저를 선언\n","\n","tree_tokenizer.tokenize(text)"]},{"cell_type":"markdown","metadata":{"id":"CkvuKgTVWBLz"},"source":["#### 1-1-2 문장 토큰화\n","\n","+ nltk.sent_tokenize를 사용하면 마침표에 따라 여러 문장들을 문장 단위로 구분할 수 있습니다\n","+ sent_tokenize는 단순히 마침표의 유무 만으로 문장의 종결여부를 판단하지는 않습니다 <br>\n","따라서 단순히 '.' 이 있다고해서 무조건 구분하는 것은 아닙니다"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0odqSdkGu8zx"},"outputs":[],"source":["sentences = \"NLP stands for Natural Language Processing. \\\n","It is a branch of artificial intelligence (AI) that focuses on the interaction between computers and human language. \\\n","NLP combines techniques from linguistics, computer science, and machine learning to enable computers to understand, interpret, \\\n","and generate human language in a way that is meaningful and useful.\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f-diXhd6vbAx"},"outputs":[],"source":["nltk.sent_tokenize(sentences) # 문장단위로 분리"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yZAmbe5yva_U"},"outputs":[],"source":["sentences = \"I am actively looking for Ph.D. students. and you are a Ph.D student.\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hudPioiCva9p"},"outputs":[],"source":["nltk.sent_tokenize(sentences) # 문장단위로 분리"]},{"cell_type":"markdown","metadata":{"id":"nyHb3zLGYCI0"},"source":["### 1-2. Cleaning\n","\n","> 코퍼스 내에서 토큰화 작업에 방해가 되거나 의미가 없는 부분의 텍스트, 노이즈를 제거하는 작업입니다. 토큰화 전에 정제를 하기도 하지만, 이후에도 여전히 남아있는 노이즈들을 제거하기 위해 지속적으로 수행합니다. 이때 노이즈는 특수 문자 같은 아무 의미도 갖지 않는 글자들을 의미하기도 하지만, 분석하고자 하는 목적에 맞지 않는 불필요한 단어들을 말합니다\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gFegG6TwZrYM"},"outputs":[],"source":["import nltk\n","from nltk.corpus import stopwords\n","nltk.download('stopwords') # nltk 내장 stopword 다운로드\n","\n","stop=set(stopwords.words('english'))\n","print(stop)"]},{"cell_type":"markdown","metadata":{"id":"cT94NGjsZ5i2"},"source":["nltk에서는 사전에 정의된 세트를 갖고있으며 쉽게 로드하여 사용할 수 있습니다. 이를 활용해 불용어를 제외한 단어들을 쉽게 추출합니다"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o8vDBmyGva6U"},"outputs":[],"source":["sen=\"I want to go to shopping and a I want to buy some of snack\"\n","tokens=nltk.word_tokenize(sen)\n","\n","clean_tokens=[]\n","for tok in tokens:\n","  if len(tok.lower())>1 and (tok.lower() not in stop): # 토큰의 길이가 1 초과이고, stopword에 속하지 않는 경우\n","    clean_tokens.append(tok) # clean_tokens 리스트에 추가\n","\n","\n","print(\"불용어 포함: \",tokens)\n","print(\"불용어 미포함: \",clean_tokens)"]},{"cell_type":"markdown","metadata":{"id":"_x5l8ESdaKr1"},"source":["### 1-3. Normalization\n","\n","> Normalization은 의미가 중복되거나 의미론적으로 유사한 단어들을 하나로 통합하거나, 단어의 원형을 찾아 통일해주는 작업을 말합니다. 크게 Stemming과 Lemmatization이 있습니다\n"]},{"cell_type":"markdown","metadata":{"id":"zBs0EOTlaigT"},"source":["#### 1-3-1 Stemming\n","* stem:(식물의) 줄기라는 뜻을 가진 영어단어로, 언어학에서는 stem을 어간이라고 합니다\n","* 어간은 굴절하는 단어에서 변화하지 않는 부분을 의미합니다. 즉, stemming이란 어간 추출을 말합니다\n","\n","* 쉽게 말해 형태가 변한 단어로부터 군더더기를 제거하고 그 단어의 원래 모습을 추출하는 것을 말합니다\n","\n","* 예를 들어 going이라는 단어가 있다면 Stemming을 진행할시 go가 됩니다. Computers라는 단어를 Stemming을 진행할 시 Comput 를 추출하는 과정을 Stemming이라고 합니다\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hsHGewxabzFP"},"outputs":[],"source":["from nltk.stem import PorterStemmer\n","from nltk.tokenize import word_tokenize"]},{"cell_type":"markdown","metadata":{"id":"fCsIy1KKb_fT"},"source":["우선 stemming을 하기 전에 문장들을 단어 단위로 토큰화 시켜줍니다"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m4S816DL5XYZ"},"outputs":[],"source":["s = PorterStemmer() # stemmer 선언\n","text = \"This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes.\"\n","words = word_tokenize(text)\n","print(words)"]},{"cell_type":"markdown","metadata":{"id":"mfC5zvGFcYDR"},"source":["토큰화된 단어들에 대해서 stemming을 진행합니다. 이때 stemming은 rule 기반의 알고리즘으로 이루어지기 때문에 등록이 되어있지 않은 단어의 경우이거나 잘못인식되는 경우 부정확할 수 있는 여지가 남아있습니다"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OwLNh18M5XWA"},"outputs":[],"source":["for i in words:\n","  print(f'{i} ==> {s.stem(i)}') # stemming되기 이전단어와 이후 단어를 출력"]},{"cell_type":"markdown","metadata":{"id":"ATi4c8H_crT-"},"source":["#### 1-3-2 Lemmatization\n","\n","* Lemmatization이란 문장 속에서 다양한 형태로 활용된(inflected) 단어의 표제어(lemma)를 찾는 일을 뜻합니다\n","* 여기서 말하는 표제어란 사전에서 단어의 뜻을 찾을 때 쓰는 기본형이라고 생각하시면 됩니다\n","* 즉 Lemmatization은 단어의 원형을 추출해주는 녀석입니다\n","* 예를들어, is를 Lemmatization하면 be가 되고, ate을 Lemmatization하면 eat이 됩니다"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PeDYjCrgdC3t"},"outputs":[],"source":["from nltk.stem import WordNetLemmatizer\n","nltk.download('wordnet')"]},{"cell_type":"markdown","metadata":{"id":"jS4gYCIodEk_"},"source":["Lemmatization을 위해 WordNetLemmatizer를 로드하고, 여러 단어들을 나열해보겠습니다"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vzynRIDu5XT_"},"outputs":[],"source":["lemmatizer = WordNetLemmatizer() # lemmatizer 선언\n","words = ['doing', 'has', 'going', 'loves', 'lives', 'flying', 'dies', 'watching', 'started', 'seen']"]},{"cell_type":"markdown","metadata":{"id":"l1sviF3kiCA6"},"source":["WordNetLemmatizer는 입력으로 단어가 동사 품사라는 사실을 알려줄 수 있습니다. 즉, 단어들이 문장에서 동사로 쓰였다는 것을 알려준다면 표제어 추출기는 품사의 정보를 보존하면서 정확한 Lemma를 출력하게 됩니다"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5r3QOHKlhh70"},"outputs":[],"source":["for i in words:\n","  lemma = lemmatizer.lemmatize(i, pos='v')\n","  print(f'{i} ==> {lemma}') # lemmatization되기 이전단어와 이후 단어를 출력"]},{"cell_type":"markdown","metadata":{"id":"lcAuVXUbp53t"},"source":["### 1-4. Edit Distance\n","\n","> Edit Distance란 2개의 문자열이 얼만큼 다른가를 거리개념으로 치환해 숫자로 표현한 것이라고 이해할 수 있습니다. Edit Distance에는 삽입, 삭제, 교체로 크게 3가지 연산이 존재합니다"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0693fFHuqjf-"},"outputs":[],"source":["import nltk\n","from nltk.metrics import edit_distance"]},{"cell_type":"markdown","metadata":{"id":"Gur_0DIuqr6c"},"source":["CAT 과  HAT 두단어의 차이는 각 단어의 첫글자인 C와H 입니다. 즉 1개의 문자만이 차이가 있습니다. 따라서 CAT과 HAT의 Edit Distance는 1 입니다"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WbmNKcjZqllo"},"outputs":[],"source":["print(edit_distance(\"CAT\",\"HAT\"))"]},{"cell_type":"markdown","metadata":{"id":"juWXuKDqo40j"},"source":["### (예제) HTML 크롤링후 단어 빈도수 구하기"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ISAEhGwnpLkQ"},"outputs":[],"source":["import nltk\n","import urllib\n","import re\n","from bs4 import BeautifulSoup\n","from nltk.corpus import stopwords\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import random\n","\n","#nltk.download('averaged_perceptron_tagger')\n","nltk.download('stopwords')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uAB8poObo-_o"},"outputs":[],"source":["response=urllib.request.urlopen('http://python.org/') # 웹에 정보를 요청한 후, 돌려받은 응답을 저장하여 ‘응답 객체(HTTPResponse)’를 반환\n","html=response.read()\n","clean=BeautifulSoup(html,'html.parser').get_text() # html 코드 정제 진행\n","\n","tokens=[]\n","for tok in clean.split():\n","  tokens.append(tok) # tokens 리스트에 토큰들을 추가\n","\n","stop=set(stopwords.words('english')) # 불용어 선언\n","\n","clean_tokens=[]\n","for tok in tokens:\n","  if len(tok.lower())>1 and (tok.lower() not in stop): # 길이가 1 초과이며 stop word가 아닌 것만 추출해\n","    clean_tokens.append(tok) # clean_tokens 리스트에 추가\n","\n","Freq_dist_nltk=nltk.FreqDist(clean_tokens) # FreqDist 클래스는 문서에 사용된 단어(토큰)의 사용빈도 정보를 담는 클래스\n","Freq_dist_nltk.plot(30, cumulative=False) # 상위 30개를 시각화. 누적 갯수 표시는 False"]},{"cell_type":"markdown","metadata":{"id":"4by_4X6tvaX6"},"source":["## Required Package\n","\n","+ nltk == 3.8.1\n","\n"]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
