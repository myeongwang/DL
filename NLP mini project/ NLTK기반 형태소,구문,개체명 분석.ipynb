{"cells":[{"cell_type":"markdown","metadata":{"id":"UKjg766IvMdI"},"source":["# NLTK기반 자연언어처리 실습"]},{"cell_type":"markdown","metadata":{"id":"OXUZV0j3nBJd"},"source":["## 실습 개요\n","\n","\n","(실습) NLTK기반 자연언어처리 실습하기 <br>\n","1) 실습 목적 <br>\n"," 이번 실습에서는 NLTK라이브러리를 활용해 자연언어처리의 기본이되는 여러 분석들을 직접 실행해봅니다<br>\n"," 형태소 분석, 구문분석, 의미분석, 개체명 인식 등을 실습해봅니다\n","\n"," 2) 수강 목표\n","  - NLTK 라이브러리를 활용하여 자연언어처리의 기본적인 분석들을 수행할 수 있다\n","  - 여러 분석들의 쓰임새와 사용 방법을 알고 분석들간의 차이를 설명할 수 있다\n","  - 분석 목적에 따른 적합한 분석방법을 선정해 적용할 수 있다\n","  - 지금까지 배운 내용들을 활용해 NLTK 기반 키워드 빈도분석을 실행할 수 있다"]},{"cell_type":"markdown","metadata":{"id":"H5CYPnoeDoRu"},"source":["### 실습 목차\n","\n","* 1. NLTK기반 자연언어처리\n","  * 1-1. 어휘 분석\n","  * 1-2. 구문 분석\n","  * 1-3. 개체명 인식\n","\n","* 2. NLTK를 활용한 키워드 분석"]},{"cell_type":"markdown","metadata":{"id":"siW2vxW_5R8u"},"source":["## 1. NLTK기반 자연언어 처리\n","\n","```\n","💡 목차 개요 : NLTK 라이브러리를 이용해 자연언어 처리의 여러 분석들을 손쉽게 적용해봅니다\n","```\n","\n","- 1-1 문장의 단어들을 대상으로 형태소를 분석해 분리하고, 각 단어들의 품사를 태깅해 원하는 속성의 단어들만 추출합니다\n","- 1-2 단어간의 의존관계나 문장의 구조를 구구조분석을 통해 파악합니다\n","- 1-3 텍스트내에서 개체명을 인식하고 어떤 단어가 어떤 유형에 속하는지 반환합니다\n"]},{"cell_type":"markdown","metadata":{"id":"wOhHH9j78vg0"},"source":["### 1-1. 어휘분석\n","\n","> 문장내에 있는 여러 단어들을 형태소 기준으로 분리하거나 언어학적으로 어떤 품사에 해당하는지를 분석합니다. 또한 konlpy를 이용해 한국어에서 사용되는 여러 형태소 분석기를 알아보고 차이점을 확인합니다\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jLRXK4HLrxFA"},"outputs":[],"source":["import nltk\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')"]},{"cell_type":"markdown","metadata":{"id":"BLeskjL4VvGq"},"source":["**pos_tag**\n","+ nltk의 pos_tag는 분리된 단어들에 대해서 형태소 분석 후 알맞는 품사를 태깅해서 보여줍니다\n","+ 따라서 input으로는 분리된 단어 리스트가 입력되어야하며 output으로는 tuple형태의 `(형태소, 품사)` 형태로 반환됩니다"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FfGxHIhXrxCq"},"outputs":[],"source":["sentence=\"My name is jack and i am 22 years old and my major is natural language processing\"\n","\n","tokens = nltk.word_tokenize(sentence) # 토크나이저로 분리\n","tagged = nltk.pos_tag(tokens) # pos tagging 실행\n","\n","for i,j in tagged:\n","  print(f'{i} - {j}')"]},{"cell_type":"markdown","metadata":{"id":"9Nsq35wkfZeO"},"source":["여기서는 pos_tag 함수를 이용해 원하는 품사의 형태소만 추출합니다"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NvwEB5ZHfWCH"},"outputs":[],"source":["allnoun = []\n","for word, pos in tagged:\n","  if pos in ['NN', 'NNP']: # 품사태그 중 NN, NNP만 추출\n","    allnoun.append(word)\n","\n","print(\"형태소 분석 결과: \",tagged)\n","print(\"명사만 추출: \", allnoun)\n","\n","allverb = []\n","for word, pos in tagged:\n","  if pos in ['VBZ','VBD']: # 품사태그 중 VBZ, VBD만 추출\n","    allverb.append(word)\n","\n","print(\"동사만 추출\",allverb)"]},{"cell_type":"markdown","metadata":{"id":"glQPYQrIfrF4"},"source":["이번에는 한국어에서의 형태소 분석 및 품사 태깅을 해봅니다. 영어는 일반적으로 형태소의 기준이 space를 기준으로 하다보니 별도의 형태소 분석기가 없어도 어느정도 수행이 가능하지만, 한국어는 교착어로써 형태소 분석기가 꼭 필요합니다. 이를 위해 Konlpy를 설치합니다"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CBWvWLAwf8QY"},"outputs":[],"source":["# konlpy 패키지 다운로드\n","!pip install konlpy"]},{"cell_type":"markdown","metadata":{"id":"N0UzLq3XgERP"},"source":["konlpy에서는 여러 종류의 형태소 분석기들을 제공합니다. 각 형태소 분석기 별로 정하고있는 분석 규칙이 다르기 때문에 같은 문장이라도 다르게 형태소를 분석합니다"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s3Ee1oxFgAV_"},"outputs":[],"source":["# konlpy 관련 패키지 import\n","from konlpy.tag import Okt\n","from konlpy.tag import Kkma\n","from konlpy.tag import Hannanum\n","from konlpy.tag import Komoran\n","from konlpy.tag import Twitter"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lR2XMgwCgISf"},"outputs":[],"source":["kkma = Kkma()\n","okt = Okt()\n","komoran = Komoran()\n","hannanum = Hannanum()\n","twitter = Twitter()"]},{"cell_type":"markdown","metadata":{"id":"ZxW7TC14gV2h"},"source":["**morphs** 함수를 이용하면 형태소 분석을 수행할 수 있습니다"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AN8pxXGDgMF-"},"outputs":[],"source":["# konlpy 의 라이브러리 형태소 분석 비교\n","print(\"okt 형태소 분석 :\", okt.morphs(u\"집에 가면 감자 좀 쪄줄래?\"))\n","print(\"kkma 형태소 분석 : \", kkma.morphs(u\"집에 가면 감자 좀 쪄줄래?\"))\n","print(\"hannanum 형태소 분석 : \", hannanum.morphs(u\"집에 가면 감자 좀 쪄줄래?\"))\n","print(\"komoran 형태소 분석 : \", komoran.morphs(u\"집에 가면 감자 좀 쪄줄래?\"))\n","print(\"twitter 형태소 분석 : \", twitter.morphs(u\"집에 가면 감자 좀 쪄줄래?\"))"]},{"cell_type":"markdown","metadata":{"id":"TiGm6qgzgpiG"},"source":["마찬가지로 한국어 형태소 분석기도 **pos** 함수를 통해 품사 태깅을 할 수 있습니다"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6L9s33bJgo6d"},"outputs":[],"source":["# konlpy 의 라이브러리 품사태깅 비교\n","print(\"okt 품사태깅 :\", okt.pos(u\"집에 가면 감자 좀 쪄줄래?\"))\n","print(\"kkma 품사태깅 : \", kkma.pos(u\"집에 가면 감자 좀 쪄줄래?\"))\n","print(\"hannanum 품사태깅 : \", hannanum.pos(u\"집에 가면 감자 좀 쪄줄래?\"))\n","print(\"komoran 품사태깅 : \", komoran.pos(u\"집에 가면 감자 좀 쪄줄래?\"))\n","print(\"twitter 품사태깅 : \", twitter.pos(u\"집에 가면 감자 좀 쪄줄래?\"))"]},{"cell_type":"markdown","metadata":{"id":"Mv5_edJLhM49"},"source":["**nouns** 함수를 이용하면 konlpy에서는 별도의 지정없이 사용하는 형태소 분석기에서 명사를 추출할 수 있습니다"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mUDs4fXRg4LK"},"outputs":[],"source":["komoran.nouns('집에 가면 감자 좀 쪄줄래?')"]},{"cell_type":"markdown","metadata":{"id":"nyHb3zLGYCI0"},"source":["### 1-2. 구문 분석\n","\n","> 구문 분석은 명사구, 동사구 처럼 구문구조를 분석하는 것을 의미합니다. 문장을 이루고 있는 구, 단어들이 문장 내에서 어떤 수식, 의존 관계를 갖는지 나타내는 일종의 Tree를 만들어주는 거라고 생각하면 이해하기 쉽습니다\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H2Bo28HQjPc6"},"outputs":[],"source":["import konlpy\n","import nltk"]},{"cell_type":"markdown","metadata":{"id":"Zsmq8oaOj41-"},"source":["구문 분석을 위해 우선 형태소 분석을 먼저 수행합니다. 형태소 분석해 단어들의 품사를 모두 태깅합니다"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5L5qB54tiBhn"},"outputs":[],"source":["sentence = '만 6세 이하의 초등학교 취학 전 자녀를 양육하기 위해서는'\n","words = konlpy.tag.Twitter().pos(sentence) # 품사태깅\n","words"]},{"cell_type":"markdown","metadata":{"id":"Wuot-JXUj_zY"},"source":["다음으로 명사, 동사, 형용사 구의 규칙을 사전에 정의해줍니다. 이는 앞서 수행한 형태소 분석에 따른 품사에 따라 규칙을 정의해줍니다"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o8vDBmyGva6U"},"outputs":[],"source":["# 명사, 동사, 형용사 구의 규칙을 정의합니다\n","grammar = \"\"\"\n","NP: {<N.*>*<Suffix>?}   # Noun phrase\n","VP: {<V.*>*}            # Verb phrase\n","AP: {<A.*>*}            # Adjective phrase\n","\"\"\"\n","parser = nltk.RegexpParser(grammar) # 선언된 grammer rule을 parser에게 입력\n","chunks = parser.parse(words) # grammer rule에 따라 단어를 파싱\n","print(\"# Print whole tree\")\n","print(chunks.pprint())\n","\n","# 명사구에 해당하는 subtree만 출력합니다\n","print(\"\\n# Print noun phrases only\")\n","for subtree in chunks.subtrees():\n","    if subtree.label()=='NP': # 서브트리의 품사가 NP인 경우\n","        print(' '.join((e[0] for e in list(subtree))))\n","        print(subtree.pprint())\n"]},{"cell_type":"markdown","metadata":{"id":"6r_-KEJLkayJ"},"source":["구문 분석은 NLTK 뿐만아니라 Spacy라는 라이브러리에서도 손쉽게 수행이 가능합니다. 부가적으로 Spacy는 영어 형태소 분석기로써 NLTK만큼 자주 사용되는 라이브러리로 구문분석에 있어서 NLTK보다 직관적인 장점이있기 때문에, 이번 실습 여기 부분에서만 잠깐 다루어 보겠습니다"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R76hsJpYkmpV"},"outputs":[],"source":["import spacy\n","nlp = spacy.load('en_core_web_sm')"]},{"cell_type":"markdown","metadata":{"id":"Cv5ppt5vk0aM"},"source":["Spacy는 문장을 token들로 구성된 document로 처리합니다. 각 token에는 품사, 의존 관계, 개체명 정보 등이 태깅되어 어떤 토큰이 어떤것을 수식하고 있는지 등의 관계를 알 수 있습니다"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ybqr-lcukq9A"},"outputs":[],"source":["doc = nlp('The fat cat sat on the mat')\n","for token in doc:\n","  print(token.text, token.dep_, token.head.text) # 토큰, 의존관계, 토큰 / 두 토큰 사이의 의존관계를 표시"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eKG5cKOnlCuO"},"outputs":[],"source":["spacy.displacy.render(doc, style='dep', jupyter=True)"]},{"cell_type":"markdown","metadata":{"id":"_x5l8ESdaKr1"},"source":["### 1-3. 개체명 인식\n","\n","> 개체명 인식(Named Entity Recognition)이란 말 그대로 이름을 가진 개체를 인식하겠다는 것을 의미합니다. 어떤 이름을 의미하는 단어를 보고는 그 단어가 어떤 유형에 속하는 단어인지를 인식하는 것을 말합니다\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hsHGewxabzFP"},"outputs":[],"source":["from nltk import word_tokenize, pos_tag, ne_chunk\n","nltk.download('maxent_ne_chunker')\n","nltk.download('words')"]},{"cell_type":"markdown","metadata":{"id":"fCsIy1KKb_fT"},"source":["ne_chunk를 통해 단어들의 개체명을 추출할 수 있습니다. ne_chunk는 개체명을 태깅하기 위해서 앞서 품사 태깅(pos_tag)이 수행되어야 합니다. 결과에서 Mark는 PERSON(사람), Stanford University는 조직(ORGANIZATION), California은 위치(GPE)라고  개체명 인식이 수행된 것을 볼 수 있습니다"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m4S816DL5XYZ"},"outputs":[],"source":["sent = \"Mark is studying at Stanford University in California\"\n","\n","tokens = word_tokenize(sent) # 토크나이징\n","tagged = pos_tag(tokens) # 품사태깅\n","entities = ne_chunk(tagged) # 개체명인식\n","\n","print(entities)"]},{"cell_type":"markdown","metadata":{"id":"XN5aAfJYuqvn"},"source":["## 2. NLTK를 활용한 키워드 분석\n","\n","```\n","💡 목차 개요 : NLTK 라이브러리를 이용해 corpus에서 빈도기반 키워드 분석을 실행해 봅니다\n","```\n","\n","- 문자열 corpus내에서 형태소 분석을 통해 명사, 형용사만을 추출해 빈도수 그래프와 워드 클라우드를 생성합니다"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2ppzVwfdv2dn"},"outputs":[],"source":["import pandas as pd\n","from tqdm import tqdm\n","from collections import Counter\n","\n","import matplotlib.pyplot as plt\n","from wordcloud import WordCloud\n","\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tag import pos_tag\n","from nltk import word_tokenize\n","nltk.download('book', quiet=True)\n","nltk.download('punkt', quiet=True)\n","nltk.download('stopwords')\n","\n","nltk.corpus.gutenberg.fileids() # nltk.corpus.gutenberg 는 여러 텍스트 코퍼스들을 제공"]},{"cell_type":"markdown","metadata":{"id":"CU8tK2C91OiE"},"source":["nltk에서 제공하는 book corpus 중 여기서는 austen-emma라는 corpus를 사용하겠습니다. lower() 함수로 모두 소문자로 만들어줍니다"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"APNtjzLnw3Dy"},"outputs":[],"source":["data = nltk.corpus.gutenberg.raw('austen-emma.txt').lower()"]},{"cell_type":"markdown","metadata":{"id":"Ts8qxqjM1Zah"},"source":["corpus를 word_tokenize 함수를 통해 형태소 분석을 수행합니다"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W9NgfcY4yWde"},"outputs":[],"source":["word_list = word_tokenize(data)"]},{"cell_type":"markdown","metadata":{"id":"LTiZMMou1hJm"},"source":["extract_by_tag는 word_list와 tag_name을 입력받아 word_list에 대해 원하는 품사 태그를 tag_name에 입력하면 해당 품사에 해당하는 단어들만 추출이 가능한 함수입니다. sorting은 Counter 모듈을 이용해 단어를 빈도수기반으로 정렬해줍니다"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RjjYxN_nv-Pa"},"outputs":[],"source":["# 위에서 추출된 morphs를 통해 원하는 품사에 해당하는 단어들만 추출해 리스트로 반환하는 함수\n","def extract_by_tag(word_list, tag_name):\n","  pos_list = pos_tag(word_list, tagset='universal') # 여기서는 universal tag를 이용\n","  extracted_list = []\n","  for word_tag in tqdm(pos_list):\n","    if tag_name in word_tag[1]: # 0번은 품사 태그. 1번은 텍스트로 여기서는 텍스트만 추출\n","      extracted_list.append(word_tag[0])\n","  return extracted_list\n","\n","# 추출된 리스트를 출현단어 빈도수를 매핑하여 정렬\n","def sorting(sentence_list):\n","  count = Counter(sentence_list)\n","  words = (dict(count.most_common()))\n","  return words"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rFf6x1n7zsAQ"},"outputs":[],"source":["nouns = extract_by_tag(word_list,'NOUN') # 명사만 추출\n","print(f'명사 예시:{nouns[5:10]}')\n","\n","adjs = extract_by_tag(word_list,'ADJ') # 형용사만 추출\n","print(f'\\n형용사 예시:{adjs[:5]}')"]},{"cell_type":"markdown","metadata":{"id":"RLfpQyIF3wko"},"source":["nltk에서 정의된 불용어들을 명사, 형용사 리스트에서 제외하는 정제작업을 거칩니다.\n","\n","그 후 sorting을 통해 높은 빈도수를 갖는 단어 순으로 정렬해줍니다"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r1djKQWs10z1"},"outputs":[],"source":["# 제거할 불용어를 정의\n","stop = stopwords.words('english')\n","stop.extend(['mr.','mrs.']) # 원하는 불용어 추가\n","\n","# 지정한 불용어를 각각 리스트에서 제거함\n","nouns = [token for token in nouns if not token in stop]\n","adjs = [token for token in adjs if not token in stop]\n","\n","# 추출된 리스트를 출현단어 빈도수를 매핑하여 정렬\n","nouns_count = sorting(nouns)\n","adj_count = sorting(adjs)"]},{"cell_type":"markdown","metadata":{"id":"dlbPuakM4DPB"},"source":["마지막으로 Wordcloud와 빈도기반 키워드 그래프를 생성합니다. WordCloud 함수에 generate_from_frequencies를 사용해 Counter 클래스로 묶인 인자를 넣으면 자동으로 워드클라우드를 쉽게 만들 수 있습니다"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QdRG0mXB3lWp"},"outputs":[],"source":["def view_wordcloud(counter_class):\n","  # 단어 빈도수를 기준으로 WordCloud를 생성함\n","  wordcloud = WordCloud(background_color='white',\n","                      colormap = \"Accent_r\",width=2500, height=1500,\n","                      stopwords=stopwords\n","                      ).generate_from_frequencies(counter_class)\n","  plt.figure(figsize=(10,8)) # 이미지 출력사이즈 지정\n","  plt.imshow(wordcloud)\n","  plt.tight_layout(pad=0)\n","  plt.axis('off')\n","  plt.show()\n","\n","  # 상위 20개 키워드의 출현 빈도수를 그래프로 표현\n","  plt.figure(figsize=(10, 5)) # 이미지 출력사이즈 지정\n","  plt.xlabel('keywords') # x축 이름지정\n","  plt.ylabel('counts') # y축 이름지정\n","  plt.xticks(rotation=70) # x축 틱 각도 회전\n","  for key, value in zip(list(counter_class.keys())[:20],list(counter_class.values())[:20]): # 상위 20개만 추출\n","      plt.bar(key, value, color='lightgrey') # barchart 생성\n","  plt.show()\n","\n","  # 단어별 빈도수 데이터 프레임 생성\n","  counter_df = pd.DataFrame({'word':counter_class.keys(),'counts':counter_class.values()})\n","\n","  display(counter_df)\n","  return counter_df # return 값으로 단어별 빈도수 데이터프레임을 반환"]},{"cell_type":"markdown","metadata":{"id":"ltdnAUhs4S2I"},"source":["빈도기반 키워드 그래프의 경우 현재 함수에서는 상위 20개에 대해서만 생성하고있습니다. 워드클라우드와 그래프가 키워드 분석 결과를 시각적으로 표현하고 있는 것을 확인할 수 있습니다. 또한 return 값으로 데이터 프레임 형태도 반환하고 있어, 추후 자유롭게 사용할 수 있습니다"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lO1E41L43h7j"},"outputs":[],"source":["nouns_df = view_wordcloud(nouns_count)"]},{"cell_type":"markdown","metadata":{"id":"uCh_Ufxp4lnl"},"source":["다음은 형용사 단어들에 대한 결과입니다"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"98SO3_oZ4lYz"},"outputs":[],"source":["adj_df = view_wordcloud(adj_count)"]},{"cell_type":"markdown","metadata":{"id":"4by_4X6tvaX6"},"source":["## Required Package\n","\n","+ nltk == 3.8.1\n","+ konlpy == 0.6.0\n","+ spacy == 3.5.3\n","\n"]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
