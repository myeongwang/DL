{"cells":[{"cell_type":"markdown","metadata":{"id":"UKjg766IvMdI"},"source":["# RNN 계열 실습"]},{"cell_type":"markdown","metadata":{"id":"OXUZV0j3nBJd"},"source":["\n","1) 실습 목적 <br>\n","이번 실습에서는 연속적인 데이터를 처리하기 위한 구조로, Transformer 등장 이전에 가장 활발히 사용된 RNN (Recurrent Neural Network)과 RNN의 변형인  LSTM (Long Short-Term Memory), GRU (Gated Recurrent Unit)에 대해 pytorch를 사용하여 구현하고 이해해봅니다<br>\n","더 나아가, 연속적인 데이터를 활용하여 이를 학습해보고, RNN에서 발생하는 Gradient Vanishing / Exploding 문제를 직접 확인해보고 LSTM과 GRU가 이를 극복할 수 있는지 직접 확인해봅니다 <br>\n","\n","</font></b>\n","\n"," 2) 수강 목표\n","  * RNN, LSTM, GRU를 구현하고 작동 방식을 이해한다\n","  * One-to-one / One-to-many / Many-to-one / Many-to-many RNN을 구현하고 각 구조를 이해한다\n","  * 각 과정에서 일어나는 연산과 input/output 형태에 대해 이해한다\n","  * RNN에서 발생하는 Gradient Vanishing / Exploding 현상을 직접 관찰하고, LSTM과 GRU가 극복 가능한지 확인한다\n","  * RNN, LSTM, GRU를 이해하고 사용할 수 있다\n"]},{"cell_type":"markdown","metadata":{"id":"H5CYPnoeDoRu"},"source":["### 실습 목차\n","* 1. RNN 실습\n","  * 1-1. RNN 구현\n","  * 1-2. RNN 학습\n","* 2. LSTM과 GRU 실습\n","  * 2-1. LSTM과 GRU 구현\n","  * 2-2. LSTM과 GRU 학습\n","* 3. One-to-one / One-to-many / Many-to-one / Many-to-many RNNs 구현 및 실습\n","  * 3-1. One-to-one RNN 실습\n","  * 3-2. One-to-Many RNN 실습\n","  * 3-3. Many-to-One RNN 실습\n","  * 3-4. Many-to-Many RNN 실습\n","* 4. Gradient Vanishing / Exploding"]},{"cell_type":"markdown","metadata":{"id":"Dh-zD-6MHpiV"},"source":["**※ 코드를 실행 전에 상단에 `런타임` →  `런타임 유형 변경` →  `하드웨어 가속기`를 `GPU`로 설정해주세요!**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KsoJ5HGx3eFX"},"outputs":[],"source":["# 라이브러리 import\n","import torch\n","import torch.nn as nn\n","import numpy as np\n","\n","# GPU 설정\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"markdown","metadata":{"id":"siW2vxW_5R8u"},"source":["## 1. RNN 실습\n","\n","```\n","💡 목차 개요 : pytorch를 통해 RNN을 구현 및 학습해보고, RNN의 동작 원리를 이해합니다\n","```\n","\n","- 1-1. RNN 구현\n","- 1-2. RNN 학습\n"]},{"cell_type":"markdown","metadata":{"id":"IU3T6nT43vfM"},"source":["#### RNN 구현\n","\n","- 이 코드는 간단한 RNN 모델을 정의합니다.\n","  - `__init__` 함수에서는 필요한 레이어를 정의하고\n","  - `forward` 함수에서는 이 레이어들을 어떻게 연결할지를 정의합니다"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jLRXK4HLrxFA"},"outputs":[],"source":["class SimpleRNN(nn.Module): # SimpleRNN 클래스 선언\n","    def __init__(self, n_inputs, n_hidden, n_outputs):\n","        super(SimpleRNN, self).__init__() # nn.Module의 초기화 함수 상속\n","        self.M = n_hidden # 은닉 상태(hidden state)의 크기를 지정\n","        self.D = n_inputs # 입력 차원의 크기 지정\n","        self.K = n_outputs # 출력 차원의 크기 지정\n","        self.rnn = nn.RNN( # RNN 모듈을 생성\n","            input_size=self.D, # 입력 차원의 크기 지정\n","            hidden_size=self.M, # 은닉 상태의 크기 지정\n","            nonlinearity='tanh', # 활성화 함수로 tanh를 사용\n","            batch_first=True) # 배치 차원이 먼저 오도록 설정\n","        self.fc = nn.Linear(self.M, self.K) # 출력을 위한 선형 변환을 정의\n","\n","    def forward(self, X): # 순전파 함수를 정의\n","        # initial hidden states\n","        h0 = torch.zeros(1, X.size(0), self.M).to(X.device) # 초기 은닉 상태를 0으로 설정\n","\n","        # get RNN unit output\n","        out, _ = self.rnn(X, h0) # RNN에 입력을 전달하고 출력을 받음\n","\n","        # we only want h(T) at the final time step\n","        out = self.fc(out[:, -1, :]) # 마지막 시간 단계의 출력만 사용하여 선형 변환을 수행\n","\n","        return out"]},{"cell_type":"markdown","metadata":{"id":"Z3UwyY7I36Yb"},"source":["#### RNN 학습\n","\n","RNN을 학습시키기 위해서는 먼저 데이터를 준비해야 합니다. 여기서는 간단한 예제를 위해 임의의 데이터를 사용하겠습니다.\n","\n","이 코드는 RNN 모델을 학습시키는 과정을 보여줍니다. 각 에포크 (epoch)에서는 입력 데이터의 시퀀스를 순회하면서 모델의 출력과 실제 출력을 비교하여 손실 (loss)을 계산합니다. 그런 다음 이 손실을 이용하여 모델의 가중치를 업데이트합니다.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11094,"status":"ok","timestamp":1693623238660,"user":{"displayName":"Chanjun(박찬준)","userId":"17071894424200158822"},"user_tz":-540},"id":"FfGxHIhXrxCq","outputId":"e92ce484-0b52-41cb-b49e-98dc8ddf537e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [30/300], Loss: 0.1273\n","Epoch [60/300], Loss: 0.0485\n","Epoch [90/300], Loss: 0.0270\n","Epoch [120/300], Loss: 0.0179\n","Epoch [150/300], Loss: 0.0130\n","Epoch [180/300], Loss: 0.0100\n","Epoch [210/300], Loss: 0.0081\n","Epoch [240/300], Loss: 0.0067\n","Epoch [270/300], Loss: 0.0057\n","Epoch [300/300], Loss: 0.0049\n"]}],"source":["model = SimpleRNN(n_inputs=2, n_hidden=20, n_outputs=2).to(device) # SimpleRNN 모델을 생성하고, GPU로 전송\n","criterion = nn.CrossEntropyLoss() # 손실 함수로 CrossEntropyLoss를 사용\n","optimizer = torch.optim.Adam(model.parameters()) # 최적화 알고리즘으로 Adam을 사용\n","\n","# 더미 입력 데이터 예제\n","inputs = torch.from_numpy(np.array([[[1, 2], [3, 4], [5, 6]]], dtype=np.float32)).to(device)\n","\n","for epoch in range(300): # 300번의 에폭 동안 학습을 진행\n","    model.zero_grad() # 기울기를 0으로 초기화\n","    outputs = model(inputs) # 모델에 입력을 전달하고 출력을 받음\n","    loss = criterion(outputs, torch.tensor([1]).to(device))  # 더미 타겟 데이터로 손실(loss)을 계산\n","    loss.backward() # 역전파를 통해 기울기를 계산\n","    optimizer.step() # 최적화 알고리즘을 통해 파라미터를 업데이트\n","\n","    if (epoch+1) % 30 == 0: # 30 에폭마다 손실을 출력\n","        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, 300, loss.item()))"]},{"cell_type":"markdown","metadata":{"id":"mQykx0ME4czH"},"source":["\n","## 2. LSTM과 GRU 실습\n","\n","```\n","💡 목차 개요 : pytorch를 통해 RNN의 변형인 LSTM과 GRU를 구현 및 학습해보고, 동작 원리를 이해합니다\n","```\n","- 2-1. LSTM과 GRU 구현\n","- 2-2. LSTM과 GRU 학습\n"]},{"cell_type":"markdown","metadata":{"id":"Uczemf3h4fKe"},"source":["##### LSTM과 GRU 구현\n","\n","\n","LSTM(Long Short-Term Memory)과 GRU(Gated Recurrent Unit)는 RNN의 변형으로, gradient vanishing 또는 exploding 문제를 완화하는 메커니즘이 추가되었습니다"]},{"cell_type":"markdown","metadata":{"id":"rZs3sqI-fllj"},"source":["LSTM은 `torch.nn.LSTM`을 사용하여 구현할 수 있습니다\n","\n","`torch.nn.LSTM`의 초기화 인자는 다음과 같습니다:\n","\n","\n","- `input_size` : LSTM 모듈의 입력 차원의 크기를 지정합니다. 이는 각 입력 요소 벡터의 크기를 의미합니다. 여기서 self.D는 입력 차원의 크기를 나타내는 변수입니다\n","\n","- `hidden_size` : LSTM 모듈의 은닉 상태의 크기를 지정합니다. 이는 LSTM의 은닉층의 뉴런 수를 의미합니다. 여기서 self.M는 은닉 상태의 크기를 나타내는 변수입니다\n","\n","- `num_layers` : LSTM 셀을 몇 층으로 쌓을 것인지를 결정합니다. 기본값은 1입니다\n","\n","- `batch_first` : 입력 데이터의 형태를 결정하는 옵션입니다. True로 설정하면 입력 데이터의 형태가 (배치 크기, 시퀀스 길이, 입력 차원)이 됩니다. False로 설정하면 (시퀀스 길이, 배치 크기, 입력 차원)이 됩니다\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x-QhZRCq4gd2"},"outputs":[],"source":["class LSTM(nn.Module): # LSTM 클래스 선언\n","    def __init__(self, n_inputs, n_hidden, n_outputs):\n","        super(LSTM, self).__init__() # nn.Module의 초기화 함수 상속\n","        self.D = n_inputs # 입력 차원의 크기 지정\n","        self.M = n_hidden # 은닉 상태(hidden state)의 크기를 지정\n","        self.K = n_outputs # 출력 차원의 크기 지정\n","        self.lstm = nn.LSTM( # LSTM 모듈을 생성\n","            input_size=self.D, # 입력 차원의 크기 지정\n","            hidden_size=self.M, # 은닉 상태의 크기 지정\n","            batch_first=True) # 배치 차원이 먼저 오도록 설정\n","        self.fc = nn.Linear(self.M, self.K) # 출력을 위한 선형 변환을 정의\n","\n","    def forward(self, X): # 순전파 함수를 정의\n","        h0 = torch.zeros(1, X.size(0), self.M).to(X.device) # 초기 은닉 상태를 0으로 설정\n","        c0 = torch.zeros(1, X.size(0), self.M).to(X.device) # LSTM의 초기 cell state를 0으로 설정\n","\n","        # get RNN unit output\n","        out, _ = self.lstm(X, (h0, c0)) # LSTM에 입력을 전달하고 출력을 받음\n","\n","        # we only want h(T) at the final time step\n","        out = self.fc(out[:, -1, :]) # 마지막 시간 단계의 출력만 사용하여 선형 변환을 수행\n","\n","        return out"]},{"cell_type":"markdown","metadata":{"id":"IY1hSLJPfpHz"},"source":["GRU는 `torch.nn.GRU`을 사용하여 구현할 수 있습니다\n","\n","`torch.nn.GRU`의 초기화 인자는 다음과 같습니다:\n","\n","- `input_size` : GRU 모듈의 입력 차원의 크기를 지정합니다. 이는 각 입력 요소 벡터의 크기를 의미합니다\n","\n","- `hidden_size` : GRU 모듈의 은닉 상태의 크기를 지정합니다. 이는 GRU의 은닉층의 뉴런 수를 의미합니다\n","\n","- `num_layers` : GRU 셀을 몇 층으로 쌓을 것인지를 결정합니다. 기본값은 1입니다\n","\n","- `batch_first` : 입력 데이터의 형태를 결정하는 옵션입니다. True로 설정하면 입력 데이터의 형태가 (배치 크기, 시퀀스 길이, 입력 차원)이 됩니다. False로 설정하면 (시퀀스 길이, 배치 크기, 입력 차원)이 됩니다\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gKiK644p4gIW"},"outputs":[],"source":["class GRU(nn.Module): # GRU 클래스 선언\n","    def __init__(self, n_inputs, n_hidden, n_outputs):\n","        super(GRU, self).__init__() # nn.Module의 초기화 함수 상속\n","        self.D = n_inputs # 입력 차원의 크기 지정\n","        self.M = n_hidden # 은닉 상태(hidden state)의 크기를 지정\n","        self.K = n_outputs # 출력 차원의 크기 지정\n","        self.gru = nn.GRU( # GRU 모듈을 생성\n","            input_size=self.D, # 입력 차원의 크기 지정\n","            hidden_size=self.M, # 은닉 상태의 크기 지정\n","            batch_first=True) # 배치 차원이 먼저 오도록 설정\n","        self.fc = nn.Linear(self.M, self.K) # 출력을 위한 선형 변환을 정의\n","\n","    def forward(self, X): # 순전파 함수를 정의\n","        # initial hidden states\n","        h0 = torch.zeros(1, X.size(0), self.M).to(X.device) # 초기 은닉 상태를 0으로 설정\n","\n","        # get RNN unit output\n","        out, _ = self.gru(X, h0) # GRU에 입력을 전달하고 출력을 받음\n","\n","        # we only want h(T) at the final time step\n","        out = self.fc(out[:, -1, :]) # 마지막 시간 단계의 출력만 사용하여 선형 변환을 수행\n","\n","        return out"]},{"cell_type":"markdown","metadata":{"id":"Kqg2qqD94kx2"},"source":["#### LSTM과 GRU 학습"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":701,"status":"ok","timestamp":1693623540838,"user":{"displayName":"Chanjun(박찬준)","userId":"17071894424200158822"},"user_tz":-540},"id":"0VfU-fyj4n7d","outputId":"f19dfe01-17cb-4d5b-e116-0ebcec09aee8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [30/300], Loss: 0.2008\n","Epoch [60/300], Loss: 0.0616\n","Epoch [90/300], Loss: 0.0230\n","Epoch [120/300], Loss: 0.0117\n","Epoch [150/300], Loss: 0.0073\n","Epoch [180/300], Loss: 0.0052\n","Epoch [210/300], Loss: 0.0039\n","Epoch [240/300], Loss: 0.0031\n","Epoch [270/300], Loss: 0.0026\n","Epoch [300/300], Loss: 0.0022\n"]}],"source":["# LSTM 학습\n","model = LSTM(n_inputs=2, n_hidden=20, n_outputs=2).to(device) # LSTM 모델을 생성\n","criterion = nn.CrossEntropyLoss() # 손실 함수로 CrossEntropyLoss를 사용\n","optimizer = torch.optim.Adam(model.parameters()) # 최적화 알고리즘으로 Adam을 사용\n","\n","for epoch in range(300): # 300회의 에포크동안 학습을 진행\n","    model.zero_grad() # 기울기를 0으로 초기화\n","    outputs = model(inputs) # 모델에 입력을 전달하고 출력을 받음\n","    loss = criterion(outputs, torch.tensor([1]).to(device))  # 예시로 사용할 목표 텐서 생성\n","    loss.backward() # 역전파를 수행하여 기울기를 계산\n","    optimizer.step() # 최적화 알고리즘을 통해 파라미터 업데이트\n","\n","    if (epoch+1) % 30 == 0: # 30 에포크마다 손실을 출력\n","        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, 300, loss.item()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":531,"status":"ok","timestamp":1693623558621,"user":{"displayName":"Chanjun(박찬준)","userId":"17071894424200158822"},"user_tz":-540},"id":"rhfIsYb_4ny1","outputId":"73ae0ebb-989b-4faa-8235-2bcfb9e1ae3b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [30/300], Loss: 0.3474\n","Epoch [60/300], Loss: 0.1188\n","Epoch [90/300], Loss: 0.0548\n","Epoch [120/300], Loss: 0.0333\n","Epoch [150/300], Loss: 0.0233\n","Epoch [180/300], Loss: 0.0176\n","Epoch [210/300], Loss: 0.0139\n","Epoch [240/300], Loss: 0.0113\n","Epoch [270/300], Loss: 0.0094\n","Epoch [300/300], Loss: 0.0079\n"]}],"source":["# GRU 학습\n","model = GRU(n_inputs=2, n_hidden=20, n_outputs=2).to(device) # GRU 모델 인스턴스 생성\n","criterion = nn.CrossEntropyLoss() # 손실 함수로 CrossEntropyLoss를 사용\n","optimizer = torch.optim.Adam(model.parameters()) # 최적화 알고리즘으로 Adam을 사용\n","\n","for epoch in range(300):\n","    model.zero_grad()\n","    outputs = model(inputs)\n","    loss = criterion(outputs, torch.tensor([1]).to(device))  # A dummy target example\n","    loss.backward()\n","    optimizer.step()\n","\n","    if (epoch+1) % 30 == 0:\n","        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, 300, loss.item()))"]},{"cell_type":"markdown","metadata":{"id":"di1Zcy0eNu43"},"source":["\n","## 3. One-to-one / One-to-many / Many-to-one / Many-to-many RNNs 구현 및 실습\n","\n","```\n","💡 목차 개요 : pytorch를 통해 One-to-one / One-to-many / Many-to-one / Many-to-many RNNs을 구현하고 학습하여 동작 원리를 이해합니다\n","```\n","- 3-1. One-to-one RNN 실습\n","- 3-2. One-to-Many RNN 실습\n","- 3-3. Many-to-One RNN 실습\n","- 3-4. Many-to-Many RNN 실습"]},{"cell_type":"markdown","metadata":{"id":"ExcPJpVp4wGU"},"source":["#### One-to-one RNN 실습\n","One-to-one RNN을 구현해보고 학습해봅시다\n","\n","<img src=\"https://github.com/js-lee-AI/assets/assets/60927808/6edb2a60-009f-4219-aaea-02df147389b1\" height=\"300\">\n","\n","\n","# One-to-One\n","이 모델은 각 입력에 대해 하나의 출력을 생성하는 가장 기본적인 RNN 구조입니다.\n","예를 들어, 주어진 숫자의 제곱을 예측하는 문제를 해결할 수 있습니다.\n","각 입력 숫자에 대해 하나의 출력(입력 숫자의 제곱)을 생성합니다.\n","One-to-one 구조의 RNN 모델은 주식 가격 예측 등에 사용될 수 있습니다"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8019,"status":"ok","timestamp":1693623698958,"user":{"displayName":"Chanjun(박찬준)","userId":"17071894424200158822"},"user_tz":-540},"id":"OwBuNfjf4xjE","outputId":"5b8e77cf-6881-4433-f67b-0243e0677a63"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [100/4000], Loss: 50.7856\n","Epoch [200/4000], Loss: 25.2362\n","Epoch [300/4000], Loss: 19.1932\n","Epoch [400/4000], Loss: 14.2959\n","Epoch [500/4000], Loss: 9.1926\n","Epoch [600/4000], Loss: 5.3577\n","Epoch [700/4000], Loss: 3.4169\n","Epoch [800/4000], Loss: 2.7019\n","Epoch [900/4000], Loss: 2.4273\n","Epoch [1000/4000], Loss: 2.2608\n","Epoch [1100/4000], Loss: 2.1212\n","Epoch [1200/4000], Loss: 1.9904\n","Epoch [1300/4000], Loss: 1.8619\n","Epoch [1400/4000], Loss: 1.7322\n","Epoch [1500/4000], Loss: 1.5993\n","Epoch [1600/4000], Loss: 1.4623\n","Epoch [1700/4000], Loss: 1.3213\n","Epoch [1800/4000], Loss: 1.1776\n","Epoch [1900/4000], Loss: 1.0332\n","Epoch [2000/4000], Loss: 0.8910\n","Epoch [2100/4000], Loss: 0.7542\n","Epoch [2200/4000], Loss: 0.6258\n","Epoch [2300/4000], Loss: 0.5086\n","Epoch [2400/4000], Loss: 0.4046\n","Epoch [2500/4000], Loss: 0.3148\n","Epoch [2600/4000], Loss: 0.2395\n","Epoch [2700/4000], Loss: 0.1781\n","Epoch [2800/4000], Loss: 0.1294\n","Epoch [2900/4000], Loss: 0.0918\n","Epoch [3000/4000], Loss: 0.0636\n","Epoch [3100/4000], Loss: 0.0430\n","Epoch [3200/4000], Loss: 0.0283\n","Epoch [3300/4000], Loss: 0.0182\n","Epoch [3400/4000], Loss: 0.0114\n","Epoch [3500/4000], Loss: 0.0069\n","Epoch [3600/4000], Loss: 0.0041\n","Epoch [3700/4000], Loss: 0.0024\n","Epoch [3800/4000], Loss: 0.0013\n","Epoch [3900/4000], Loss: 0.0007\n","Epoch [4000/4000], Loss: 0.0004\n","Input: 2.0, Output: 4.005894184112549, 정답: 4.0\n"]}],"source":["# One-to-One\n","X = np.random.randint(1, 5, size=(1000, 1, 1)) # 입력 데이터 생성, 1~4 사이의 정수 1000개를 랜덤하게 생성\n","Y = np.square(X) # 타겟 데이터 생성, 입력 데이터의 제곱을 타겟으로 설정\n","\n","X = torch.from_numpy(X.astype(np.float32)).to(device) # 입력 데이터를 텐서로 변환\n","Y = torch.from_numpy(Y.astype(np.float32)).squeeze(-1).to(device) # 타겟 데이터를 텐서로 변환\n","\n","model = SimpleRNN(n_inputs=1, n_hidden=40, n_outputs=1).to(device) # 모델 생성\n","criterion = nn.MSELoss() # 손실 함수 설정\n","optimizer = torch.optim.Adam(model.parameters()) # 최적화 알고리즘 설정\n","\n","for epoch in range(4000): # 4000회 반복하여 학습\n","    model.zero_grad() # 기울기 초기화\n","    outputs = model(X) # 모델에 입력 데이터 전달하여 예측값 계산\n","    loss = criterion(outputs, Y) # 예측값과 타겟값을 이용하여 손실 계산\n","    loss.backward() # 역전파 수행\n","    optimizer.step() # 가중치 업데이트\n","\n","    if (epoch+1) % 100 == 0: # 100회마다 손실 출력\n","        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, 4000, loss.item()))\n","\n","# Inference\n","X_test = torch.tensor([[[2.0]]], dtype=torch.float32).to(device) # 테스트 데이터 생성\n","print(f\"Input: 2.0, Output: {model(X_test).item()}, 정답: {np.square(2.0)}\") # 테스트 데이터에 대한 예측값 출력"]},{"cell_type":"markdown","metadata":{"id":"Olk3epVh5dwV"},"source":["#### One-to-Many RNN 실습\n","One-to-Many RNN을 구현해보고 학습해봅시다\n","\n","<img src=\"https://github.com/js-lee-AI/assets/assets/60927808/8dac773c-b0f8-417d-b3f2-7705c9e14e51\" width=\"500\" height=\"300\">\n","\n","# One-to-Many\n","One-to-Many 구조의 모델은 하나의 입력에 대해 여러 개의 출력을 생성하는 구조입니다.\n","예를 들어, 주어진 숫자의 배수를 예측하는 문제를 해결할 수 있습니다.\n","하나의 입력 숫자에 대해 여러 개의 출력(입력 숫자의 배수)을 생성합니다.\n","이러한 모델은 이미지 캡셔닝(하나의 이미지에 대한 여러 단어의 설명 생성), 음성 합성(하나의 텍스트 입력에 대한 여러 개의 음성 출력 생성) 등에 사용됩니다"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6701,"status":"ok","timestamp":1693623827475,"user":{"displayName":"Chanjun(박찬준)","userId":"17071894424200158822"},"user_tz":-540},"id":"380y5Sim5fo-","outputId":"97d07c17-4a20-46da-fd49-3f110204bad8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [100/4000], Loss: 208.2682\n","Epoch [200/4000], Loss: 115.4876\n","Epoch [300/4000], Loss: 69.8474\n","Epoch [400/4000], Loss: 49.2465\n","Epoch [500/4000], Loss: 38.6924\n","Epoch [600/4000], Loss: 31.7193\n","Epoch [700/4000], Loss: 25.4721\n","Epoch [800/4000], Loss: 19.3838\n","Epoch [900/4000], Loss: 14.0058\n","Epoch [1000/4000], Loss: 9.8412\n","Epoch [1100/4000], Loss: 6.9928\n","Epoch [1200/4000], Loss: 5.2137\n","Epoch [1300/4000], Loss: 4.1409\n","Epoch [1400/4000], Loss: 3.4691\n","Epoch [1500/4000], Loss: 3.0042\n","Epoch [1600/4000], Loss: 2.6453\n","Epoch [1700/4000], Loss: 2.3467\n","Epoch [1800/4000], Loss: 2.0875\n","Epoch [1900/4000], Loss: 1.8563\n","Epoch [2000/4000], Loss: 1.6464\n","Epoch [2100/4000], Loss: 1.4538\n","Epoch [2200/4000], Loss: 1.2761\n","Epoch [2300/4000], Loss: 1.1120\n","Epoch [2400/4000], Loss: 0.9611\n","Epoch [2500/4000], Loss: 0.8231\n","Epoch [2600/4000], Loss: 0.6981\n","Epoch [2700/4000], Loss: 0.5862\n","Epoch [2800/4000], Loss: 0.4874\n","Epoch [2900/4000], Loss: 0.4014\n","Epoch [3000/4000], Loss: 0.3276\n","Epoch [3100/4000], Loss: 0.2653\n","Epoch [3200/4000], Loss: 0.2136\n","Epoch [3300/4000], Loss: 0.1712\n","Epoch [3400/4000], Loss: 0.1370\n","Epoch [3500/4000], Loss: 0.1097\n","Epoch [3600/4000], Loss: 0.0882\n","Epoch [3700/4000], Loss: 0.0714\n","Epoch [3800/4000], Loss: 0.0583\n","Epoch [3900/4000], Loss: 0.0481\n","Epoch [4000/4000], Loss: 0.0400\n","-------------------- 추론 결과 --------------------\n","Input: 2.0\n","Output: 1.9, 정답: 2\n","Output: 3.9, 정답: 4\n","Output: 5.9, 정답: 6\n","Output: 7.9, 정답: 8\n","Output: 9.9, 정답: 10\n","Output: 11.9, 정답: 12\n","Output: 13.9, 정답: 14\n","Output: 15.9, 정답: 16\n","Output: 17.9, 정답: 18\n","Output: 19.9, 정답: 20\n"]}],"source":["#### 입력의 배수 10개를 타겟으로 설정\n","# 아래의 One-to-Many는 하나의 숫자 입력 데이터를 받고, 입력된 데이터의 배수 10개를 예측하는 RNN 모델을 구현합니다.\n","\n","# One-to-Many\n","class SimpleRNNOne2Many(nn.Module): # One-to-Many 모델 클래스 선언\n","    def __init__(self, n_inputs, n_hidden, n_outputs):\n","        super(SimpleRNNOne2Many, self).__init__()\n","        self.D = n_inputs\n","        self.M = n_hidden\n","        self.K = n_outputs\n","        self.rnn = nn.RNN(\n","            input_size=self.D,\n","            hidden_size=self.M,\n","            nonlinearity='tanh',\n","            batch_first=True) # RNN 모듈 생성\n","        self.fc = nn.Linear(self.M, self.K) # 선형 변환 정의\n","\n","    def forward(self, X): # 순전파 함수 정의\n","        h0 = torch.zeros(1, X.size(0), self.M).to(X.device) # 초기 은닉 상태를 0으로 설정\n","        out, _ = self.rnn(X, h0) # RNN에 입력을 전달하고 출력을 받음\n","        out = self.fc(out) # 출력에 선형 변환을 수행\n","        return out.view(-1, 10) # 출력을 적절한 형태로 변환\n","\n","X = np.random.randint(1, 5, size=(1000, 1, 1)) # 입력 데이터 생성, 1~4 사이의 정수 1000개를 랜덤하게 생성\n","Y = np.array([[i*j for i in range(1, 11)] for j in X.squeeze()]) # 타겟 데이터 생성, 입력의 배수 10개를 타겟으로 설정\n","\n","X = torch.from_numpy(X.astype(np.float32)).to(device) # 입력 데이터를 텐서로 변환\n","Y = torch.from_numpy(Y.astype(np.float32)).to(device) # 타겟 데이터를 텐서로 변환\n","\n","model = SimpleRNNOne2Many(n_inputs=1, n_hidden=40, n_outputs=10).to(device) # 모델 생성\n","criterion = nn.MSELoss() # 손실 함수 설정\n","optimizer = torch.optim.Adam(model.parameters()) # 최적화 알고리즘 설정\n","\n","for epoch in range(4000): # 4000회 반복하여 학습\n","    model.zero_grad() # 기울기 초기화\n","    outputs = model(X) # 모델에 입력 데이터 전달하여 예측값 계산\n","    loss = criterion(outputs.squeeze(), Y) # 예측값과 타겟값을 이용하여 손실 계산\n","    loss.backward() # 역전파 수행\n","    optimizer.step() # 가중치 업데이트\n","\n","    if (epoch+1) % 100 == 0: # 100회마다 손실 출력\n","        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, 4000, loss.item()))\n","\n","# Inference\n","X_test = torch.tensor([[[2.0]]], dtype=torch.float32).to(device) # 테스트 데이터 생성\n","print('-' * 20, '추론 결과', '-' * 20)\n","print(f\"Input: 2.0\")\n","output = [round(num, 1) for num in model(X_test).squeeze().tolist()] # 테스트 데이터에 대한 예측값 계산\n","answer = list(range(2, 21, 2)) # 정답 리스트 생성\n","\n","for o, a in zip(output, answer): # 예측값과 정답을 비교하여 출력\n","    print(f\"Output: {o}, 정답: {a}\")"]},{"cell_type":"markdown","metadata":{"id":"U6eP93U15hfm"},"source":["#### Many-to-One RNN 실습\n","Many-to-one RNN을 구현해보고 학습해봅시다\n","\n","<img src=\"https://github.com/js-lee-AI/assets/assets/60927808/0b844a30-397a-4af2-b06d-163a7aa06a14\" width=\"400\">\n","\n","# Many-to-One\n","Many-to-One 구조의 모델은 여러 개의 입력에 대해 하나의 출력을 생성합니다.\n","예를 들어, 주어진 숫자 리스트의 합을 예측하는 문제를 해결할 수 있습니다.\n","여러 개의 입력 숫자에 대해 하나의 출력(입력 숫자의 합)을 생성합니다.\n","Many-to-One 구조의 모델은 감성 분석(여러 단어로 이루어진 텍스트에 대한 하나의 감성 점수 예측), 스팸 메일 분류(메일의 여러 단어에 대한 하나의 스팸/비스팸 레이블 예측) 등에 사용될 수 있습니다"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":46238,"status":"ok","timestamp":1693623950796,"user":{"displayName":"Chanjun(박찬준)","userId":"17071894424200158822"},"user_tz":-540},"id":"wZ8ggTD75jvG","outputId":"23bd76bb-f821-4ffc-d84f-036ebe3d36ba"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [100/4000], Loss: 1491.8268\n","Epoch [200/4000], Loss: 1128.6877\n","Epoch [300/4000], Loss: 854.6137\n","Epoch [400/4000], Loss: 643.7408\n","Epoch [500/4000], Loss: 483.1585\n","Epoch [600/4000], Loss: 363.1685\n","Epoch [700/4000], Loss: 275.6174\n","Epoch [800/4000], Loss: 213.4687\n","Epoch [900/4000], Loss: 170.6896\n","Epoch [1000/4000], Loss: 142.2226\n","Epoch [1100/4000], Loss: 123.9612\n","Epoch [1200/4000], Loss: 112.6982\n","Epoch [1300/4000], Loss: 106.0355\n","Epoch [1400/4000], Loss: 102.2638\n","Epoch [1500/4000], Loss: 100.2245\n","Epoch [1600/4000], Loss: 99.1730\n","Epoch [1700/4000], Loss: 98.6559\n","Epoch [1800/4000], Loss: 98.4115\n","Epoch [1900/4000], Loss: 98.2956\n","Epoch [2000/4000], Loss: 98.2216\n","Epoch [2100/4000], Loss: 93.2784\n","Epoch [2200/4000], Loss: 58.8609\n","Epoch [2300/4000], Loss: 35.4755\n","Epoch [2400/4000], Loss: 25.5550\n","Epoch [2500/4000], Loss: 19.6364\n","Epoch [2600/4000], Loss: 15.5970\n","Epoch [2700/4000], Loss: 12.6368\n","Epoch [2800/4000], Loss: 10.4157\n","Epoch [2900/4000], Loss: 8.7053\n","Epoch [3000/4000], Loss: 7.3556\n","Epoch [3100/4000], Loss: 6.2831\n","Epoch [3200/4000], Loss: 5.3979\n","Epoch [3300/4000], Loss: 4.7919\n","Epoch [3400/4000], Loss: 4.0786\n","Epoch [3500/4000], Loss: 3.5767\n","Epoch [3600/4000], Loss: 3.1527\n","Epoch [3700/4000], Loss: 2.7910\n","Epoch [3800/4000], Loss: 2.5114\n","Epoch [3900/4000], Loss: 2.2129\n","Epoch [4000/4000], Loss: 1.9804\n","-------------------- 추론 결과 --------------------\n","Input: [2.0, 4.0, 6.0, 8.0, 10.0, 11.0]\n","Output: 41.1, 정답: 41.0\n"]}],"source":["# 아래의 Many-to-One은 여러 개의 숫자 입력 데이터를 받고, 입력된 데이터의 총 합을 예측하는 RNN 모델을 구현합니다.\n","# Many-to-One\n","class ManyToOneRNN(nn.Module): # Many-to-One 모델 클래스 선언\n","    def __init__(self, input_size, hidden_size, output_size):\n","        super(ManyToOneRNN, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True) # RNN 모듈 생성\n","        self.fc = nn.Linear(hidden_size, output_size) # 선형 변환 정의\n","\n","    def forward(self, x): # 순전파 함수 정의\n","        h0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device) # 초기 은닉 상태를 0으로 설정\n","        out, _ = self.rnn(x, h0) # RNN에 입력을 전달하고 출력을 받음\n","        out = self.fc(out[:, -1, :]) # 마지막 시간 단계의 출력만 사용하여 선형 변환을 수행\n","\n","        return out\n","\n","X = np.random.randint(1, 15, size=(50000, 6, 1)) # 입력 데이터 생성, 1~14 사이의 정수 50000개를 랜덤하게 생성\n","Y = np.array([np.sum(x) for x in X]) # 타겟 데이터 생성, 모든 입력 데이터의 총합이 타겟\n","\n","X = torch.from_numpy(X.astype(np.float32)).to(device) # 입력 데이터를 텐서로 변환\n","Y = torch.from_numpy(Y.astype(np.float32)).to(device) # 타겟 데이터를 텐서로 변환\n","\n","model = ManyToOneRNN(input_size=1, hidden_size=50, output_size=1).to(device) # 모델 생성\n","criterion = nn.MSELoss() # 손실 함수 설정\n","optimizer = torch.optim.Adam(model.parameters()) # 최적화 알고리즘 설정\n","\n","for epoch in range(4000):\n","    model.zero_grad()\n","    outputs = model(X)\n","    loss = criterion(outputs.squeeze(), Y)\n","    loss.backward()\n","    optimizer.step()\n","\n","    if (epoch+1) % 100 == 0:\n","        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, 4000, loss.item()))\n","\n","# Inference\n","X_test = torch.tensor([[[2.0], [4.0], [6.0], [8.0], [10.0], [11.0]]], dtype=torch.float32).to(device)\n","print('-' * 20, '추론 결과', '-' * 20)\n","print(f\"Input: {X_test.squeeze().tolist()}\")\n","output = round(model(X_test).item(), 1)\n","answer = round(sum(X_test.squeeze().tolist()), 1)\n","\n","print(f\"Output: {output}, 정답: {answer}\")"]},{"cell_type":"markdown","metadata":{"id":"CgQoEIOM5kpG"},"source":["## Many-to-Many RNN 실습\n","Many-to-Many RNN을 구현해보고 학습해봅시다.\n","\n","Many-to-Many 구조의 모델은 여러 개의 입력에 대해 여러 개의 출력을 생성합니다.\n","예를 들어, 주어진 숫자 리스트의 누적 합을 예측하는 문제를 해결할 수 있습니다.\n","여러 개의 입력 숫자에 대해 여러 개의 출력(입력 숫자의 누적 합)을 생성합니다.\n","이러한 모델은 기계 번역(하나의 언어로 작성된 여러 단어에 대한 다른 언어로 작성된 여러 단어의 번역 생성), 요약 (긴 문장들을 짧은 문장으로 생성) 등에 사용될 수 있습니다"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":73820,"status":"ok","timestamp":1693624083866,"user":{"displayName":"Chanjun(박찬준)","userId":"17071894424200158822"},"user_tz":-540},"id":"uEDV46i85myV","outputId":"b3e0ee63-9f71-427c-e388-ef6addc72d96"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [100/4000], Loss: 1974.8226\n","Epoch [200/4000], Loss: 1566.7333\n","Epoch [300/4000], Loss: 1266.6932\n","Epoch [400/4000], Loss: 1032.9783\n","Epoch [500/4000], Loss: 848.1955\n","Epoch [600/4000], Loss: 698.7219\n","Epoch [700/4000], Loss: 576.6282\n","Epoch [800/4000], Loss: 477.1007\n","Epoch [900/4000], Loss: 397.0463\n","Epoch [1000/4000], Loss: 332.4071\n","Epoch [1100/4000], Loss: 279.7634\n","Epoch [1200/4000], Loss: 236.6477\n","Epoch [1300/4000], Loss: 201.1883\n","Epoch [1400/4000], Loss: 171.8819\n","Epoch [1500/4000], Loss: 147.5240\n","Epoch [1600/4000], Loss: 127.1678\n","Epoch [1700/4000], Loss: 110.0757\n","Epoch [1800/4000], Loss: 95.6600\n","Epoch [1900/4000], Loss: 83.4444\n","Epoch [2000/4000], Loss: 73.0572\n","Epoch [2100/4000], Loss: 64.1904\n","Epoch [2200/4000], Loss: 56.5908\n","Epoch [2300/4000], Loss: 50.0509\n","Epoch [2400/4000], Loss: 44.4053\n","Epoch [2500/4000], Loss: 39.4806\n","Epoch [2600/4000], Loss: 35.2032\n","Epoch [2700/4000], Loss: 31.4614\n","Epoch [2800/4000], Loss: 28.1821\n","Epoch [2900/4000], Loss: 25.2961\n","Epoch [3000/4000], Loss: 22.7497\n","Epoch [3100/4000], Loss: 20.4959\n","Epoch [3200/4000], Loss: 18.4980\n","Epoch [3300/4000], Loss: 16.7208\n","Epoch [3400/4000], Loss: 15.1378\n","Epoch [3500/4000], Loss: 13.7239\n","Epoch [3600/4000], Loss: 12.4903\n","Epoch [3700/4000], Loss: 11.3309\n","Epoch [3800/4000], Loss: 10.3168\n","Epoch [3900/4000], Loss: 9.4060\n","Epoch [4000/4000], Loss: 8.5849\n","-------------------- 추론 결과 --------------------\n","Input: [10, 11, 12, 13, 14]\n","Output: 10.0, 정답: 10\n","Output: 21.0, 정답: 21\n","Output: 32.9, 정답: 33\n","Output: 45.7, 정답: 46\n","Output: 59.8, 정답: 60\n"]}],"source":["#### 입력 데이터 리스트의 누적 합 리스트가 정답\n","# 아래의 Many-to-Many 구조는 여러 개의 숫자 입력 데이터를 받고, 입력된 데이터의 누적합을 예측하는 RNN 모델을 구현합니다.\n","# 예를 들어, 1,3,5를 입력으로 받으면 RNN 모델은 이들의 누적합인 1,4,9를 출력하도록 학습됩니다.\n","\n","# Many-to-Many\n","class ManyToManyRNN(nn.Module): # ManyToManyRNN 클래스 선언\n","    def __init__(self, input_size, hidden_size, output_size):\n","        super(ManyToManyRNN, self).__init__() # nn.Module의 초기화 함수 상속\n","        self.hidden_size = hidden_size # 은닉 상태(hidden state)의 크기를 지정\n","        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True) # RNN 모듈을 생성\n","        self.fc = nn.Linear(hidden_size, output_size) # 출력을 위한 선형 변환을 정의\n","\n","    def forward(self, x): # 순전파 함수를 정의\n","        h0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device) # 초기 은닉 상태를 0으로 설정\n","        out, _ = self.rnn(x, h0) # RNN에 입력을 전달하고 출력을 받음\n","        out = self.fc(out) # 모든 시간 단계의 출력에 대해 선형 변환을 수행\n","\n","        return out\n","\n","# 데이터 생성\n","# 각각의 리스트는 0~30 사이의 랜덤한 정수를 가지는 5개의 정수로 구성, 이러한 리스트를 3000개 생성\n","X = np.array([[[np.random.randint(0, 31)] for _ in range(5)] for _ in range(3000)])\n","Y = np.array([np.cumsum(x) for x in X]) # 정답은 각 리스트의 누적합\n","\n","X = torch.from_numpy(X.astype(np.float32)) # numpy 배열을 PyTorch 텐서로 변환\n","Y = torch.from_numpy(Y.astype(np.float32)) # numpy 배열을 PyTorch 텐서로 변환\n","\n","model = ManyToManyRNN(1, 60, 1) # 모델 생성\n","\n","criterion = nn.MSELoss() # 손실 함수 설정\n","optimizer = torch.optim.Adam(model.parameters()) # 최적화 알고리즘 설정\n","\n","# 학습\n","for epoch in range(4000): # 4000번의 에폭 동안 학습\n","    model.zero_grad() # 기울기를 0으로 초기화\n","    outputs = model(X) # 모델에 입력을 전달하고 출력을 받음\n","    loss = criterion(outputs, Y.view_as(outputs)) # 손실 계산\n","    loss.backward() # 역전파 수행\n","    optimizer.step() # 가중치 갱신\n","\n","    if (epoch+1) % 100 == 0: # 100 에폭마다 손실 출력\n","        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, 4000, loss.item()))\n","\n","# 추론\n","X_test = torch.tensor([[[i+10] for i in range(5)]], dtype=torch.float32) # 테스트 데이터 생성\n","print('-' * 20, '추론 결과', '-' * 20)\n","print(f\"Input: {list(range(10, 15))}\")\n","output = [round(num, 1) for num in model(X_test).squeeze().tolist()] # 모델의 출력 계산\n","answer = list(np.cumsum(range(10, 15))) # 정답 계산\n","\n","for o, a in zip(output, answer): # 모델의 출력과 정답 비교\n","    print(f\"Output: {o}, 정답: {a}\")"]},{"cell_type":"markdown","metadata":{"id":"YvdKeKKr55oL"},"source":["## 4. Gradient Vanishing / Exploding\n","\n","```\n","💡 목차 개요 : RNN은 시퀀스가 길어질수록 처음의 정보가 끝까지 전달되지 못하는 Gradient Vanishing 문제와 Gradient가 너무 커져서 모델이 불안정해지는 Gradient Exploding 문제를 확인해보고, LSTM, GRU가 이를 완화할 수 있는지 확인해봅시다\n","```\n","\n"]},{"cell_type":"markdown","metadata":{"id":"6LbgX1ijaTXz"},"source":["RNN은 긴 시퀀스를 처리할 때 gradient vanishing 또는 exploding 문제를 겪을 수 있습니다. 이는 각 시퀀스 스텝에서 이전 스텝의 정보를 전달하는 과정에서 정보가 손실되거나, 그래디언트가 너무 커져서 파라미터가 제대로 업데이트되지 않는 문제를 의미합니다\n","\n","RNN은 시퀀스가 길어질수록 처음의 정보가 끝까지 전달되지 못하는 Gradient Vanishing 문제와 Gradient가 너무 커져서 모델이 불안정해지는 Gradient Exploding 문제가 있습니다. 이를 확인하기 위해 SimpleRNN 모델에 긴 시퀀스 데이터를 학습하고, 학습률 (learning rate)을 크게 설정하여 학습을 진행해보면, RNN이 LSTM, GRU 보다 더 높은 loss를 기록하는 것을 확인할 수 있습니다. 즉, RNN에서 Gradient Exploding / Vanishing 문제를 확인할 수 있습니다"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DTl_lVYZ57J7"},"outputs":[],"source":["# 긴 시퀀스 데이터 생성\n","N = 50  # number of samples\n","T = 20000  # sequence length\n","D = 10  # input dimensionality\n","X = np.random.randn(N, T, D)\n","\n","# Targets are binary - 0 or 1\n","Y = np.array([1 if x.mean() > 0 else 0 for x in X]) # X의 각 샘플에 대해 평균이 0보다 크면 1, 그렇지 않으면 0"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1693624139986,"user":{"displayName":"Chanjun(박찬준)","userId":"17071894424200158822"},"user_tz":-540},"id":"7uy07qtS58lT","outputId":"15a55618-d513-4729-8362-fd70e391d134"},"outputs":[{"data":{"text/plain":["(array([[[ 0.70338203, -0.22470518, -0.25881412, ..., -0.89202369,\n","           1.33822481, -0.13949196],\n","         [ 0.04594292,  0.5721611 , -1.09314781, ...,  1.23921039,\n","           0.30722024,  1.14611391],\n","         [-0.00687995,  0.14539342, -0.2169408 , ...,  2.26734485,\n","          -1.18753764,  2.29719656],\n","         ...,\n","         [ 0.01304562, -0.99217194,  0.77812961, ..., -2.23289119,\n","          -1.5966615 , -1.10824546],\n","         [-1.00994753, -0.96539265,  1.01284997, ...,  0.49545009,\n","           0.72240584, -0.1763065 ],\n","         [ 0.96279892,  0.70777894,  0.82122272, ..., -1.03420976,\n","           0.10938281, -1.57081008]],\n"," \n","        [[ 0.5305529 , -0.50448519,  0.89366606, ...,  1.11510732,\n","           1.49382758, -1.37135707],\n","         [ 0.29154474, -0.02630598,  0.50583689, ...,  0.54472606,\n","           0.63576751, -0.62527457],\n","         [-0.48608331,  0.26244451, -0.38990753, ..., -1.89786405,\n","          -0.75340474, -0.30848156],\n","         ...,\n","         [-1.23670985,  0.50729847,  0.27605733, ...,  1.44202984,\n","           1.86023518, -1.93201435],\n","         [-0.88228252,  1.10158336, -0.13710517, ..., -0.85786675,\n","           0.8597252 ,  1.21646178],\n","         [-0.87518317,  1.08342395, -0.81562944, ...,  0.13262018,\n","           0.67929756, -1.06125419]]]),\n"," array([1, 1]))"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["# 샘플 데이터 출력\n","X[:2], Y[:2]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7251,"status":"ok","timestamp":1693624185371,"user":{"displayName":"Chanjun(박찬준)","userId":"17071894424200158822"},"user_tz":-540},"id":"jHSmu5Ht59oC","outputId":"14730409-bbac-475c-be4d-60597269619b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [10/120], Loss: 0.6784\n","Epoch [20/120], Loss: 0.6453\n","Epoch [30/120], Loss: 0.6098\n","Epoch [40/120], Loss: 0.5712\n","Epoch [50/120], Loss: 0.5265\n","Epoch [60/120], Loss: 0.4760\n","Epoch [70/120], Loss: 0.4189\n","Epoch [80/120], Loss: 0.3586\n","Epoch [90/120], Loss: 0.3110\n","Epoch [100/120], Loss: 0.2744\n","Epoch [110/120], Loss: 0.2433\n","Epoch [120/120], Loss: 0.2126\n"]}],"source":["class SimpleRNN(nn.Module): # SimpleRNN 클래스 선언\n","    def __init__(self, n_inputs, n_hidden, n_outputs):\n","        super(SimpleRNN, self).__init__() # nn.Module의 초기화 함수 상속\n","        self.M = n_hidden # 은닉 상태(hidden state)의 크기를 지정\n","        self.D = n_inputs # 입력 차원의 크기 지정\n","        self.K = n_outputs # 출력 차원의 크기 지정\n","        self.rnn = nn.RNN( # RNN 모듈을 생성\n","            input_size=self.D, # 입력 차원의 크기 지정\n","            hidden_size=self.M, # 은닉 상태의 크기 지정\n","            nonlinearity='tanh', # 활성화 함수로 tanh를 사용\n","            batch_first=True) # 배치 차원이 먼저 오도록 설정\n","        self.fc = nn.Linear(self.M, self.K) # 출력을 위한 선형 변환을 정의\n","\n","    def forward(self, X): # 순전파 함수를 정의\n","        # initial hidden states\n","        h0 = torch.zeros(1, X.size(0), self.M).to(X.device) # 초기 은닉 상태를 0으로 설정\n","\n","        # get RNN unit output\n","        out, _ = self.rnn(X, h0) # RNN에 입력을 전달하고 출력을 받음\n","\n","        # we only want h(T) at the final time step\n","        out = self.fc(out[:, -1, :]) # 마지막 시간 단계의 출력만 사용하여 선형 변환을 수행\n","\n","        return out\n","\n","# RNN 학습\n","M = 5  # hidden layer size\n","K = 1  # output dimensionality\n","\n","model = SimpleRNN(n_inputs=D, n_hidden=M, n_outputs=K).to(device)\n","criterion = nn.BCEWithLogitsLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=5e-3)\n","\n","inputs = torch.from_numpy(X.astype(np.float32)).to(device)\n","targets = torch.from_numpy(Y.astype(np.float32)).to(device)\n","\n","for epoch in range(120):\n","    model.zero_grad()\n","    outputs = model(inputs)\n","    loss = criterion(outputs.squeeze(), targets)\n","    loss.backward()\n","    optimizer.step()\n","\n","    if (epoch+1) % 10 == 0:\n","        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, 120, loss.item()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7195,"status":"ok","timestamp":1693624220948,"user":{"displayName":"Chanjun(박찬준)","userId":"17071894424200158822"},"user_tz":-540},"id":"tFppJoEu5_JL","outputId":"a1ae5cd4-a331-4b6d-a2c9-f63205a71dd2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [10/120], Loss: 0.6782\n","Epoch [20/120], Loss: 0.6627\n","Epoch [30/120], Loss: 0.6376\n","Epoch [40/120], Loss: 0.5956\n","Epoch [50/120], Loss: 0.5315\n","Epoch [60/120], Loss: 0.4498\n","Epoch [70/120], Loss: 0.3682\n","Epoch [80/120], Loss: 0.2911\n","Epoch [90/120], Loss: 0.2243\n","Epoch [100/120], Loss: 0.1701\n","Epoch [110/120], Loss: 0.1336\n","Epoch [120/120], Loss: 0.1058\n"]}],"source":["class LSTM(nn.Module): # LSTM 클래스 선언\n","    def __init__(self, n_inputs, n_hidden, n_outputs):\n","        super(LSTM, self).__init__() # nn.Module의 초기화 함수 상속\n","        self.D = n_inputs # 입력 차원의 크기 지정\n","        self.M = n_hidden # 은닉 상태(hidden state)의 크기를 지정\n","        self.K = n_outputs # 출력 차원의 크기 지정\n","        self.lstm = nn.LSTM( # LSTM 모듈을 생성\n","            input_size=self.D, # 입력 차원의 크기 지정\n","            hidden_size=self.M, # 은닉 상태의 크기 지정\n","            batch_first=True) # 배치 차원이 먼저 오도록 설정\n","        self.fc = nn.Linear(self.M, self.K) # 출력을 위한 선형 변환을 정의\n","\n","    def forward(self, X): # 순전파 함수를 정의\n","        # initial hidden states\n","        h0 = torch.zeros(1, X.size(0), self.M).to(X.device) # 초기 은닉 상태를 0으로 설정\n","        c0 = torch.zeros(1, X.size(0), self.M).to(X.device) # LSTM의 초기 cell state를 0으로 설정\n","\n","        # get RNN unit output\n","        out, _ = self.lstm(X, (h0, c0)) # LSTM에 입력을 전달하고 출력을 받음\n","\n","        # we only want h(T) at the final time step\n","        out = self.fc(out[:, -1, :]) # 마지막 시간 단계의 출력만 사용하여 선형 변환을 수행\n","\n","        return out\n","\n","# LSTM 학습\n","model = LSTM(n_inputs=D, n_hidden=M, n_outputs=K).to(device)\n","criterion = nn.BCEWithLogitsLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=5e-3)\n","\n","for epoch in range(120):\n","    model.zero_grad()\n","    outputs = model(inputs)\n","    loss = criterion(outputs.squeeze(), targets)\n","    loss.backward()\n","    optimizer.step()\n","\n","    if (epoch+1) % 10 == 0:\n","        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, 120, loss.item()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7611,"status":"ok","timestamp":1693624244020,"user":{"displayName":"Chanjun(박찬준)","userId":"17071894424200158822"},"user_tz":-540},"id":"8gcf3KnB6AlT","outputId":"9eb0044e-9e78-42b8-a494-36c809bb632f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [10/120], Loss: 0.6681\n","Epoch [20/120], Loss: 0.6286\n","Epoch [30/120], Loss: 0.5801\n","Epoch [40/120], Loss: 0.5174\n","Epoch [50/120], Loss: 0.4441\n","Epoch [60/120], Loss: 0.3668\n","Epoch [70/120], Loss: 0.2941\n","Epoch [80/120], Loss: 0.2354\n","Epoch [90/120], Loss: 0.1905\n","Epoch [100/120], Loss: 0.1530\n","Epoch [110/120], Loss: 0.1218\n","Epoch [120/120], Loss: 0.0974\n"]}],"source":["class GRU(nn.Module): # GRU 클래스 선언\n","    def __init__(self, n_inputs, n_hidden, n_outputs):\n","        super(GRU, self).__init__() # nn.Module의 초기화 함수 상속\n","        self.D = n_inputs # 입력 차원의 크기 지정\n","        self.M = n_hidden # 은닉 상태(hidden state)의 크기를 지정\n","        self.K = n_outputs # 출력 차원의 크기 지정\n","        self.gru = nn.GRU( # GRU 모듈을 생성\n","            input_size=self.D, # 입력 차원의 크기 지정\n","            hidden_size=self.M, # 은닉 상태의 크기 지정\n","            batch_first=True) # 배치 차원이 먼저 오도록 설정\n","        self.fc = nn.Linear(self.M, self.K) # 출력을 위한 선형 변환을 정의\n","\n","    def forward(self, X): # 순전파 함수를 정의\n","        # initial hidden states\n","        h0 = torch.zeros(1, X.size(0), self.M).to(X.device) # 초기 은닉 상태를 0으로 설정\n","\n","        # get RNN unit output\n","        out, _ = self.gru(X, h0) # GRU에 입력을 전달하고 출력을 받음\n","\n","        # we only want h(T) at the final time step\n","        out = self.fc(out[:, -1, :]) # 마지막 시간 단계의 출력만 사용하여 선형 변환을 수행\n","\n","        return out\n","\n","# GRU 학습\n","model = GRU(n_inputs=D, n_hidden=M, n_outputs=K).to(device)\n","criterion = nn.BCEWithLogitsLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=5e-3)\n","\n","for epoch in range(120):\n","    model.zero_grad()\n","    outputs = model(inputs)\n","    loss = criterion(outputs.squeeze(), targets)\n","    loss.backward()\n","    optimizer.step()\n","\n","    if (epoch+1) % 10 == 0:\n","        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, 120, loss.item()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vAv4siLU-QTg"},"outputs":[],"source":["print(torch.__version__)\n","print(np.__version__)\n"]},{"cell_type":"markdown","metadata":{"id":"AFPHZA16lJvv"},"source":["# Reference\n","\n","- [stanford.edu](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks)"]},{"cell_type":"markdown","metadata":{"id":"4by_4X6tvaX6"},"source":["## Required Package\n","\n","+ torch == 2.0.1+cu118\n","+ numpy == 1.22.4\n","\n","\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"114hXBN9WERA-uAieVMIkO5wK7r1SCg3n","timestamp":1693155259535}],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
